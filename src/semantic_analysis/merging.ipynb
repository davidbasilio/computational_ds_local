{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')\n",
    "import semantic_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n"
     ]
    }
   ],
   "source": [
    "#perform roberta and lexicon anylsis\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2015):\n",
    "\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_{year}_output.csv\")\n",
    "\n",
    "    if os.path.exists(file_path_2):\n",
    "\n",
    "\n",
    "        semantic_analysis.lexicon_nltk(file_path_2)\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "#drop column \n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_{year}_semantics_rob.csv\")\n",
    "\n",
    "    if os.path.exists(file_path_2):\n",
    "\n",
    "\n",
    "        df = pd.read_csv(file_path_2)\n",
    "\n",
    "        df.drop(columns=['Sentiment_Lexicon'], inplace=True)\n",
    "\n",
    "        df.to_csv\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "#merge both files\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"cnn_{year}_semantics_lex.csv\")\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_{year}_semantics_rob.csv\")\n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        semantic_lex = pd.read_csv(file_path)\n",
    "        semantic_rob = pd.read_csv(file_path_2)\n",
    "\n",
    "        merged_df = pd.merge(semantic_lex, semantic_rob, on=['ID', 'Headline', 'Date', 'Organization', 'Link'] )\n",
    "\n",
    "        merged_df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\", index=False)\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "#Do roberta checked_sums\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "    \n",
    "    df['Sentiment values roberta'] = semantic_analysis.check_weighted_sum_consistency(df)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\", index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n",
      "File saved\n",
      "File saved\n",
      "File saved\n",
      "File saved\n",
      "File saved\n"
     ]
    }
   ],
   "source": [
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_final_output_{year}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path_2):\n",
    "\n",
    "\n",
    "        df = pd.read_csv(file_path_2)\n",
    "\n",
    "        df.drop(columns=['Sentiment_Lexicon_x'], inplace=True)\n",
    "\n",
    "        df.to_csv(os.path.join(base_path, f\"cnn_final_output_{year}.csv\"), index=False)\n",
    "\n",
    "        print(f\"File saved\")\n",
    "    else:\n",
    "        print(f\"No file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\merging.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m merged_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(all_data, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Save the merged dataframe to a new CSV file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m merged_data\u001b[39m.\u001b[39;49mto_csv(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(base_path, \u001b[39m\"\u001b[39;49m\u001b[39mcnn_final_output.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m), index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3721\u001b[0m     path_or_buf,\n\u001b[0;32m   3722\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3723\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3724\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3725\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3726\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3727\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3728\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3729\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3730\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3731\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3732\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3733\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3734\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3735\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3736\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3737\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:261\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[1;32m--> 261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save()\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:266\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_header()\n\u001b[1;32m--> 266\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_body()\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:304\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m start_i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m end_i:\n\u001b[0;32m    303\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_chunk(start_i, end_i)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:315\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    312\u001b[0m data \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39miget_values(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(res\u001b[39m.\u001b[39mitems))]\n\u001b[0;32m    314\u001b[0m ix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_index[slicer]\u001b[39m.\u001b[39m_format_native_types(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n\u001b[1;32m--> 315\u001b[0m libwriters\u001b[39m.\u001b[39;49mwrite_csv_rows(\n\u001b[0;32m    316\u001b[0m     data,\n\u001b[0;32m    317\u001b[0m     ix,\n\u001b[0;32m    318\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnlevels,\n\u001b[0;32m    319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcols,\n\u001b[0;32m    320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter,\n\u001b[0;32m    321\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\writers.pyx:72\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#merge cnn\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "all_data = []\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    file_path = os.path.join(base_path, f\"cnn_final_output_{year}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_data.to_csv(os.path.join(base_path, \"cnn_final_output.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REUTERS MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lexicon and some cleaning I needed to do \n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\")\n",
    "    columns_order = list(df.columns)\n",
    "    if all(col in columns_order for col in ['Semantic roberta twitter', 'Sentiment scores lexicon']):\n",
    "        # Remove 'Sentiment value roberta.2' if it already exists to re-insert it in the desired position\n",
    "        columns_order = [col for col in columns_order if col != 'Sentiment value roberta.2']\n",
    "        \n",
    "        # Find the index of 'Sentiment scores lexicon' and place 'Sentiment value roberta.2' after it\n",
    "        idx_sentiment_scores = columns_order.index('Sentiment scores lexicon')\n",
    "        columns_order.insert(idx_sentiment_scores, 'Sentiment value roberta.2')\n",
    "        \n",
    "        # Reorder the DataFrame based on the new column order\n",
    "        df = df[columns_order]\n",
    "        \n",
    "        # Save the modified DataFrame back to the CSV file\n",
    "        df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\", index=False)\n",
    "    print(f\"File saved for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also roberta didn't have the weighted sum\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\")\n",
    "\n",
    "    df.columns.values[7] = 'Sentiment label roberta'\n",
    "    \n",
    "    df['Sentiment value roberta'] = semantic_analysis.check_weighted_sum_consistency(df)\n",
    "\n",
    "    columns = list(df.columns)\n",
    "    insert_index = columns.index('Sentiment scores lexicon')  # Find the index to insert the new column\n",
    "    \n",
    "    # Insert 'Sentiment value roberta' at the appropriate position\n",
    "    columns.insert(insert_index, 'Sentiment value roberta')\n",
    "    df = df[columns]  # Reorder columns\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\", index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge reuters\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "all_data = []\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    file_path = os.path.join(base_path, f\"reuters_final_output_{year}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_data.to_csv(os.path.join(base_path, \"reuters_final_output.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE FOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n"
     ]
    }
   ],
   "source": [
    "#PERFOM LEXICON\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2017):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"fox_{year}_output.csv\")\n",
    "    \n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        semantic_analysis.lexicon_nltk(file_path)\n",
    "        #semantic_analysis.roberta_semantic_algorithm_twitter(file_path)\n",
    "        \n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"fox_{year}_semantics_lex.csv\")\n",
    "    file_path_2 = os.path.join(base_path, f\"fox_{year}_semantics_rob.csv\")\n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        semantic_lex = pd.read_csv(file_path)\n",
    "        semantic_rob = pd.read_csv(file_path_2)\n",
    "\n",
    "        merged_df = pd.merge(semantic_lex, semantic_rob, on=['ID', 'Headline', 'Date', 'Organization', 'Link'] )\n",
    "\n",
    "        merged_df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\fox_final_output_{year}.csv\", index=False)\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\fox_final_output_{year}.csv\")\n",
    "    \n",
    "    df['Sentiment values roberta'] = semantic_analysis.check_weighted_sum_consistency(df)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\fox_final_output_{year}.csv\", index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge fox\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "all_data = []\n",
    "\n",
    "for year in range(2017, 2020):\n",
    "    file_path = os.path.join(base_path, f\"fox_final_output_{year}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_data.to_csv(os.path.join(base_path, \"fox_final_output.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n",
      "File saved\n",
      "File saved\n"
     ]
    }
   ],
   "source": [
    "#drop columns sentiment_lexicon_x, sentiment_lexicon_y\n",
    "\n",
    "for year in range(2017, 2020):\n",
    "    file_path = os.path.join(base_path, f\"fox_final_output_{year}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df.drop(columns=['Sentiment_Lexicon_x'], inplace=True)\n",
    "\n",
    "        df.to_csv(os.path.join(base_path, f\"fox_final_output_{year}.csv\"), index=False)\n",
    "\n",
    "        print(f\"File saved\")\n",
    "    else:\n",
    "        print(f\"No file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE TO CLUSTER FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "\n",
    "existing_data = pd.read_csv(os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec.csv\"))\n",
    "empty_columns = ['Semantic values roberta twitter', 'Semantic roberta twitter', 'Sentiment value roberta',\n",
    "                'Sentiment scores lexicon', 'Sentiment value lexicon', 'Sentiment lexicon']\n",
    "\n",
    "\n",
    "# Add empty columns with NaN values\n",
    "for col in empty_columns:\n",
    "    existing_data[col] = pd.Series(dtype=object)\n",
    "# Save the updated DataFrame to a new file or overwrite the existing one\n",
    "\n",
    "existing_data.to_csv(os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec_updated.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Load the CSV files\n",
    "cnn_df = pd.read_csv(os.path.join(base_path,'cnn_final_output.csv'))\n",
    "reuters_df = pd.read_csv(os.path.join(base_path,'reuters_final_output.csv'))\n",
    "fox_df = pd.read_csv(os.path.join(base_path,'fox_final_output.csv'))\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "merged_df = pd.concat([cnn_df, reuters_df, fox_df], ignore_index=True)\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(os.path.join(base_path, \"merged_final_outputs.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "file_path_2 = os.path.join(base_path, f\"merged_final_outputs.csv\")\n",
    "\n",
    "if os.path.exists(file_path_2):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(file_path_2)\n",
    "\n",
    "    df.drop(columns=['Sentiment_Lexicon_x'], inplace=True)\n",
    "\n",
    "    df.to_csv(os.path.join(base_path, \"merged_final_outputs.csv\"), index=False)\n",
    "\n",
    "    print(f\"File saved\")\n",
    "else:\n",
    "    print(f\"No file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File updated\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "output_file_path = os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec_updated.csv\")\n",
    "# Load existing data\n",
    "existing_data = pd.read_csv(output_file_path)\n",
    "\n",
    "file_path_2 = os.path.join(base_path, \"merged_final_outputs.csv\")\n",
    "\n",
    "if os.path.exists(file_path_2):\n",
    "    new_data = pd.read_csv(file_path_2, usecols=lambda col: col != 'ID')\n",
    "\n",
    "    # Columns to consider in the merge\n",
    "    columns_to_merge = ['Headline', 'Organization', 'Link', 'Date', \n",
    "                        'Semantic values roberta twitter', 'Semantic roberta twitter', \n",
    "                        'Sentiment value roberta', 'Sentiment scores lexicon', \n",
    "                        'Sentiment value lexicon', 'Sentiment lexicon']\n",
    "\n",
    "    # Left merge data based on matching columns\n",
    "    merged_data = pd.merge(existing_data, new_data, on=['Headline', 'Organization', 'Link', 'Date'], how='left')\n",
    "\n",
    "    # Overwrite the existing data with merged data for this year\n",
    "    merged_data.to_csv(output_file_path, index=False)\n",
    "    print(f\"File updated\")\n",
    "else:\n",
    "    print(f\"No file found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Link</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Semantic values roberta twitter_x</th>\n",
       "      <th>Semantic roberta twitter_x</th>\n",
       "      <th>Sentiment value roberta_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Sentiment lexicon_x</th>\n",
       "      <th>Sentiment_Lexicon_x</th>\n",
       "      <th>Sentiment scores lexicon_y</th>\n",
       "      <th>Sentiment value lexicon_y</th>\n",
       "      <th>Sentiment lexicon_y</th>\n",
       "      <th>Sentiment_Lexicon_y</th>\n",
       "      <th>Semantic values roberta twitter_y</th>\n",
       "      <th>Semantic roberta twitter_y</th>\n",
       "      <th>Sentiment value roberta_y</th>\n",
       "      <th>Sentiment values roberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>China says it’s building new homegrown aircraf...</td>\n",
       "      <td>Updated 1:09 PM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights The U.S. military has long be...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/asia/china-...</td>\n",
       "      <td>619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>[0.029 0.901 0.069]</td>\n",
       "      <td>0.038</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>[0.11789864 0.75297797 0.1291234 ]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hillary Clinton emails: Kissinger, Photoshop a...</td>\n",
       "      <td>Updated 9:34 PM EST, Thu December 31, 2015</td>\n",
       "      <td>Story highlights The State Department released...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/politics/cl...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9529</td>\n",
       "      <td>[0.057 0.879 0.064]</td>\n",
       "      <td>0.007</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9529</td>\n",
       "      <td>[0.37420064 0.5465969  0.07920244]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How the stars rang in 2016</td>\n",
       "      <td>Published 9:04 AM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights Some stars hit exotic locatio...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2016/01/01/entertainme...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>[0.009 0.788 0.203]</td>\n",
       "      <td>0.194</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>[0.02461733 0.5008394  0.47454324]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Smoke still wafting from Dubai’s luxury Addres...</td>\n",
       "      <td>Updated 4:55 PM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights NEW: Fire has been contained ...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2016/01/01/middleeast/...</td>\n",
       "      <td>482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.9890</td>\n",
       "      <td>[0.102 0.849 0.049]</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.9890</td>\n",
       "      <td>[0.35776561 0.58108795 0.06114649]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Super PACs backing Cruz to launch TV ad blitz</td>\n",
       "      <td>Published 8:36 PM EST, Thu December 31, 2015</td>\n",
       "      <td>Story highlights Ads supporting Cruz set to ai...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/politics/cr...</td>\n",
       "      <td>1428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>[0.009 0.861 0.13 ]</td>\n",
       "      <td>0.121</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>[0.06863631 0.7193004  0.21206321]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>China’s one-child policy goes but heartache re...</td>\n",
       "      <td>Published 8:37 PM EST, Thu December 31, 2015</td>\n",
       "      <td>Story highlights China's two-child policy goes...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/asia/china-...</td>\n",
       "      <td>229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>[0.064 0.856 0.08 ]</td>\n",
       "      <td>0.016</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>[0.6098913  0.3609453  0.02916347]</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh court hands down death sentences fo...</td>\n",
       "      <td>Updated 5:47 AM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights Seven sentenced -- two to dea...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/asia/bangla...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.9986</td>\n",
       "      <td>[0.25  0.719 0.031]</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.9986</td>\n",
       "      <td>[0.8468856  0.1481697  0.00494466]</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Alabama, Clemson to meet in college football t...</td>\n",
       "      <td>Updated 3:22 AM EST, Fri January 1, 2016</td>\n",
       "      <td>CNN — So much for drama. Alabama and Clemson r...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/us/ncaa-foo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>[0.062 0.851 0.087]</td>\n",
       "      <td>0.025</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>[0.03096833 0.56298685 0.4060448 ]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Kanye West drops new song ‘Facts’</td>\n",
       "      <td>Updated 10:58 AM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights There may be a new Kanye albu...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2016/01/01/entertainme...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>[0.039 0.842 0.119]</td>\n",
       "      <td>0.080</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>[0.05871982 0.79267794 0.14860223]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Camille Cosby ordered to give deposition in ca...</td>\n",
       "      <td>Updated 11:17 AM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights Camille Cosby is subpoenaed i...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2016/01/01/us/camille-...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.9810</td>\n",
       "      <td>[0.123 0.819 0.058]</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.9810</td>\n",
       "      <td>[0.49592295 0.4826576  0.02141947]</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           Headline  \\\n",
       "0   1  China says it’s building new homegrown aircraf...   \n",
       "1   2  Hillary Clinton emails: Kissinger, Photoshop a...   \n",
       "2   3                         How the stars rang in 2016   \n",
       "3   4  Smoke still wafting from Dubai’s luxury Addres...   \n",
       "4   5      Super PACs backing Cruz to launch TV ad blitz   \n",
       "5   6  China’s one-child policy goes but heartache re...   \n",
       "6   7  Bangladesh court hands down death sentences fo...   \n",
       "7   8  Alabama, Clemson to meet in college football t...   \n",
       "8   9                  Kanye West drops new song ‘Facts’   \n",
       "9  10  Camille Cosby ordered to give deposition in ca...   \n",
       "\n",
       "                                           Date  \\\n",
       "0      Updated 1:09 PM EST, Fri January 1, 2016   \n",
       "1    Updated 9:34 PM EST, Thu December 31, 2015   \n",
       "2    Published 9:04 AM EST, Fri January 1, 2016   \n",
       "3      Updated 4:55 PM EST, Fri January 1, 2016   \n",
       "4  Published 8:36 PM EST, Thu December 31, 2015   \n",
       "5  Published 8:37 PM EST, Thu December 31, 2015   \n",
       "6      Updated 5:47 AM EST, Fri January 1, 2016   \n",
       "7      Updated 3:22 AM EST, Fri January 1, 2016   \n",
       "8     Updated 10:58 AM EST, Fri January 1, 2016   \n",
       "9     Updated 11:17 AM EST, Fri January 1, 2016   \n",
       "\n",
       "                                                Text Organization  \\\n",
       "0  Story highlights The U.S. military has long be...          CNN   \n",
       "1  Story highlights The State Department released...          CNN   \n",
       "2  Story highlights Some stars hit exotic locatio...          CNN   \n",
       "3  Story highlights NEW: Fire has been contained ...          CNN   \n",
       "4  Story highlights Ads supporting Cruz set to ai...          CNN   \n",
       "5  Story highlights China's two-child policy goes...          CNN   \n",
       "6  Story highlights Seven sentenced -- two to dea...          CNN   \n",
       "7  CNN — So much for drama. Alabama and Clemson r...          CNN   \n",
       "8  Story highlights There may be a new Kanye albu...          CNN   \n",
       "9  Story highlights Camille Cosby is subpoenaed i...          CNN   \n",
       "\n",
       "                                                Link  Cluster  \\\n",
       "0  https://edition.cnn.com/2015/12/31/asia/china-...      619   \n",
       "1  https://edition.cnn.com/2015/12/31/politics/cl...       -1   \n",
       "2  https://edition.cnn.com/2016/01/01/entertainme...       -1   \n",
       "3  https://edition.cnn.com/2016/01/01/middleeast/...      482   \n",
       "4  https://edition.cnn.com/2015/12/31/politics/cr...     1428   \n",
       "5  https://edition.cnn.com/2015/12/31/asia/china-...      229   \n",
       "6  https://edition.cnn.com/2015/12/31/asia/bangla...       -1   \n",
       "7  https://edition.cnn.com/2015/12/31/us/ncaa-foo...       -1   \n",
       "8  https://edition.cnn.com/2016/01/01/entertainme...       -1   \n",
       "9  https://edition.cnn.com/2016/01/01/us/camille-...       -1   \n",
       "\n",
       "   Semantic values roberta twitter_x  Semantic roberta twitter_x  \\\n",
       "0                                NaN                         NaN   \n",
       "1                                NaN                         NaN   \n",
       "2                                NaN                         NaN   \n",
       "3                                NaN                         NaN   \n",
       "4                                NaN                         NaN   \n",
       "5                                NaN                         NaN   \n",
       "6                                NaN                         NaN   \n",
       "7                                NaN                         NaN   \n",
       "8                                NaN                         NaN   \n",
       "9                                NaN                         NaN   \n",
       "\n",
       "   Sentiment value roberta_x  ...  Sentiment lexicon_x  Sentiment_Lexicon_x  \\\n",
       "0                        NaN  ...                  NaN               0.9549   \n",
       "1                        NaN  ...                  NaN               0.9529   \n",
       "2                        NaN  ...                  NaN               0.9953   \n",
       "3                        NaN  ...                  NaN              -0.9890   \n",
       "4                        NaN  ...                  NaN               0.9877   \n",
       "5                        NaN  ...                  NaN               0.9565   \n",
       "6                        NaN  ...                  NaN              -0.9986   \n",
       "7                        NaN  ...                  NaN               0.6533   \n",
       "8                        NaN  ...                  NaN               0.9781   \n",
       "9                        NaN  ...                  NaN              -0.9810   \n",
       "\n",
       "   Sentiment scores lexicon_y  Sentiment value lexicon_y Sentiment lexicon_y  \\\n",
       "0         [0.029 0.901 0.069]                      0.038             neutral   \n",
       "1         [0.057 0.879 0.064]                      0.007             neutral   \n",
       "2         [0.009 0.788 0.203]                      0.194             neutral   \n",
       "3         [0.102 0.849 0.049]                     -0.053             neutral   \n",
       "4         [0.009 0.861 0.13 ]                      0.121             neutral   \n",
       "5         [0.064 0.856 0.08 ]                      0.016             neutral   \n",
       "6         [0.25  0.719 0.031]                     -0.219             neutral   \n",
       "7         [0.062 0.851 0.087]                      0.025             neutral   \n",
       "8         [0.039 0.842 0.119]                      0.080             neutral   \n",
       "9         [0.123 0.819 0.058]                     -0.065             neutral   \n",
       "\n",
       "   Sentiment_Lexicon_y   Semantic values roberta twitter_y  \\\n",
       "0               0.9549  [0.11789864 0.75297797 0.1291234 ]   \n",
       "1               0.9529  [0.37420064 0.5465969  0.07920244]   \n",
       "2               0.9953  [0.02461733 0.5008394  0.47454324]   \n",
       "3              -0.9890  [0.35776561 0.58108795 0.06114649]   \n",
       "4               0.9877  [0.06863631 0.7193004  0.21206321]   \n",
       "5               0.9565  [0.6098913  0.3609453  0.02916347]   \n",
       "6              -0.9986  [0.8468856  0.1481697  0.00494466]   \n",
       "7               0.6533  [0.03096833 0.56298685 0.4060448 ]   \n",
       "8               0.9781  [0.05871982 0.79267794 0.14860223]   \n",
       "9              -0.9810  [0.49592295 0.4826576  0.02141947]   \n",
       "\n",
       "   Semantic roberta twitter_y Sentiment value roberta_y  \\\n",
       "0                     neutral                       NaN   \n",
       "1                     neutral                       NaN   \n",
       "2                     neutral                       NaN   \n",
       "3                     neutral                       NaN   \n",
       "4                     neutral                       NaN   \n",
       "5                    negative                       NaN   \n",
       "6                    negative                       NaN   \n",
       "7                     neutral                       NaN   \n",
       "8                     neutral                       NaN   \n",
       "9                    negative                       NaN   \n",
       "\n",
       "  Sentiment values roberta  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "5                      NaN  \n",
       "6                      NaN  \n",
       "7                      NaN  \n",
       "8                      NaN  \n",
       "9                      NaN  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(base_path, \"updated_dataframe_with_clusters_and_semantics.csv\"))\n",
    "\n",
    "\n",
    "excluded_organizations = [\"CNN\", \"FOX\", \"Reuters\"]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Organization'] not in excluded_organizations:\n",
    "        df.at[index, 'Organization'], df.at[index, 'Link'] = row['Link'], row['Organization']\n",
    "\n",
    "df.to_csv(os.path.join(base_path, 'updated_dataframe_with_clusters_and_semantics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "file_path_2 = os.path.join(base_path, \"updated_dataframe_with_clusters_and_semantics.csv\")\n",
    "\n",
    "if os.path.exists(file_path_2):\n",
    "    df = semantic_analysis.sampling_articles(file_path_2)\n",
    "\n",
    "    df.to_csv(os.path.join(base_path, \"sampling_articles.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "file_path_2 = os.path.join(base_path, \"updated_dataframe_with_clusters_and_semantics.csv\")\n",
    "\n",
    "if os.path.exists(file_path_2):\n",
    "\n",
    "    df = pd.read_csv(file_path_2)\n",
    "\n",
    "    df.isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0    ID                                           Headline  \\\n",
      "86333        86333   171        Hawaii girl, 3, dies after dental procedure   \n",
      "93274        93274  7095              Vanishing spray makes World Cup debut   \n",
      "100558      100558     1  BRIEF-Anhui Jianghuai Automobile Group Corp re...   \n",
      "100559      100559     2  BRIEF-Wi-Lan, Funai Electric enter license agr...   \n",
      "100560      100560     3  Nikkei tumbles to 2-1/2-month lows on weak Chi...   \n",
      "\n",
      "                                             Date Organization  \\\n",
      "86333   Updated 12:06 PM EST, Sun January 5, 2014          CNN   \n",
      "93274    Published 5:47 PM EDT, Thu June 12, 2014          CNN   \n",
      "100558                       2016-12-29T06:45:37Z      Reuters   \n",
      "100559                       2016-12-29T15:06:56Z      Reuters   \n",
      "100560                       2016-01-04T02:41:05Z      Reuters   \n",
      "\n",
      "                                                     Link  Cluster  \\\n",
      "86333   https://edition.cnn.com/2014/01/04/justice/haw...       -1   \n",
      "93274   https://edition.cnn.com/2014/06/12/tech/innova...       -1   \n",
      "100558      https://www.reuters.com/article/idUSL4N1EO1ZP      421   \n",
      "100559       https://www.reuters.com/article/idUSASC09P51      420   \n",
      "100560  https://www.reuters.com/article/japan-stocks-m...       -1   \n",
      "\n",
      "        Sentiment value lexicon Sentiment lexicon Semantic roberta twitter  \\\n",
      "86333                    -0.089           neutral                 negative   \n",
      "93274                     0.041           neutral                  neutral   \n",
      "100558                    0.062           neutral                  neutral   \n",
      "100559                    0.162           neutral                  neutral   \n",
      "100560                    0.073           neutral                  neutral   \n",
      "\n",
      "        Sentiment values roberta  \n",
      "86333                        NaN  \n",
      "93274                        NaN  \n",
      "100558                       NaN  \n",
      "100559                       NaN  \n",
      "100560                       NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df[df['Sentiment values roberta'].isna()][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "    \n",
    "    print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
