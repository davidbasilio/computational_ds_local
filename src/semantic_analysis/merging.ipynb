{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')\n",
    "import semantic_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2015, 2020):\n",
    "\n",
    "    file_path_1 = os.path.join(base_path, f\"cnn_{year}_semantics.csv\")\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_{year}_output.csv\")\n",
    "\n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path_2):\n",
    "\n",
    "\n",
    "        semantic_analysis.lexicon_nltk(file_path_2)\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "for year in range(2015, 2020):\n",
    "\n",
    "    file_path_1 = os.path.join(base_path, f\"cnn_{year}_semantics_lex.csv\")\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_{year}_output.csv\")\n",
    "\n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path_2):\n",
    "        \n",
    "        df = pd.read_csv(file_path_1)\n",
    "\n",
    "        main_df = pd.read_csv(file_path_2)\n",
    "\n",
    "        # Select only the columns needed from the additional DataFrame\n",
    "        columns_to_add = ['Sentiment scores lexicon', 'Sentiment value lexicon', 'Sentiment lexicon']\n",
    "        selected_additional_df = df[columns_to_add]\n",
    "\n",
    "        # Merge the DataFrames using a common identifier\n",
    "        # For example, if both DataFrames share a column 'ID':\n",
    "        merged_df = pd.merge(main_df, selected_additional_df, left_index=True, right_index=True, how='left')\n",
    "        # Save the updated DataFrame back to the CSV file\n",
    "        merged_df.to_csv(f\"cnn_final_output_{year}.csv\", index = False)\n",
    "\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "#Do roberta checked_sums\n",
    "\n",
    "for year in range(2015, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}_rob.csv\")\n",
    "\n",
    "    df.columns.values[7] = 'Sentiment label roberta'\n",
    "    \n",
    "    df['Sentiment value roberta'] = semantic_analysis.check_weighted_sum_consistency(df)\n",
    "\n",
    "    df_2 = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "\n",
    "    df_2 = df_2.combine_first(df)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df_2.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\", index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns reordered and removed for year 2015\n",
      "Columns reordered and removed for year 2016\n",
      "Columns reordered and removed for year 2017\n",
      "Columns reordered and removed for year 2018\n",
      "Columns reordered and removed for year 2019\n"
     ]
    }
   ],
   "source": [
    "#to reorder columns if needed and remove columns\n",
    "columns_order = ['ID', 'Headline', 'Date', 'Organization', 'Link', 'Semantic values roberta twitter', 'Sentiment label roberta', 'Sentiment scores lexicon', 'Sentiment value lexicon', 'Sentiment value roberta']\n",
    "\n",
    "for year in range(2015, 2020):\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}_rob.csv\")\n",
    "\n",
    "    df_2 = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "\n",
    "    # Reorder columns and remove 'Sentiment_Lexicon' and 'Text'\n",
    "    df_2 = df_2[columns_order]\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df_2.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\", index=False)\n",
    "\n",
    "    print(f\"Columns reordered and removed for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns renamed for year 2015\n",
      "Columns renamed for year 2016\n",
      "Columns renamed for year 2017\n",
      "Columns renamed for year 2018\n",
      "Columns renamed for year 2019\n"
     ]
    }
   ],
   "source": [
    "#rename a different column if need\n",
    "for year in range(2015, 2020):\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "\n",
    "    df_2 = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "\n",
    "    df_2 = df_2.rename(columns={'Sentiment label roberta': 'Semantic roberta twitter'})\n",
    "\n",
    "    df_2.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\", index=False)\n",
    "\n",
    "\n",
    "    print(f\"Columns renamed for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 done\n",
      "2016 done\n",
      "2017 done\n",
      "2018 done\n",
      "2019 done\n"
     ]
    }
   ],
   "source": [
    "#Create sentiment lexicon column which was non-existing\n",
    "\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "for year in range(2015, 2020):\n",
    "    \n",
    "    file_path = os.path.join(base_path, f\"cnn_final_output_{year}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "        existing_data['Sentiment lexicon'] = existing_data['Sentiment value lexicon'].apply(semantic_analysis.semantic_label)\n",
    "        # Save the updated DataFrame to a new file or overwrite the existing one\n",
    "        existing_data.to_csv(os.path.join(base_path, f\"cnn_final_output_{year}.csv\"), index=False)\n",
    "        print(f\"{year} done\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge CNN\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "all_data = []\n",
    "\n",
    "for year in range(2015, 2020):\n",
    "    file_path = os.path.join(base_path, f\"cnn_final_output_{year}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_data.to_csv(os.path.join(base_path, \"merged_cnn_final_output.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REUTERS MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lexicon and some cleaning I needed to do \n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\")\n",
    "    columns_order = list(df.columns)\n",
    "    if all(col in columns_order for col in ['Semantic roberta twitter', 'Sentiment scores lexicon']):\n",
    "        # Remove 'Sentiment value roberta.2' if it already exists to re-insert it in the desired position\n",
    "        columns_order = [col for col in columns_order if col != 'Sentiment value roberta.2']\n",
    "        \n",
    "        # Find the index of 'Sentiment scores lexicon' and place 'Sentiment value roberta.2' after it\n",
    "        idx_sentiment_scores = columns_order.index('Sentiment scores lexicon')\n",
    "        columns_order.insert(idx_sentiment_scores, 'Sentiment value roberta.2')\n",
    "        \n",
    "        # Reorder the DataFrame based on the new column order\n",
    "        df = df[columns_order]\n",
    "        \n",
    "        # Save the modified DataFrame back to the CSV file\n",
    "        df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\", index=False)\n",
    "    print(f\"File saved for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also roberta didn't have the weighted sum\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\")\n",
    "\n",
    "    df.columns.values[7] = 'Sentiment label roberta'\n",
    "    \n",
    "    df['Sentiment value roberta'] = semantic_analysis.check_weighted_sum_consistency(df)\n",
    "\n",
    "    columns = list(df.columns)\n",
    "    insert_index = columns.index('Sentiment scores lexicon')  # Find the index to insert the new column\n",
    "    \n",
    "    # Insert 'Sentiment value roberta' at the appropriate position\n",
    "    columns.insert(insert_index, 'Sentiment value roberta')\n",
    "    df = df[columns]  # Reorder columns\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\", index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge reuters\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "all_data = []\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    file_path = os.path.join(base_path, f\"reuters_final_output_{year}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_data.to_csv(os.path.join(base_path, \"merged_reuters_final_output.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE FOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\merging.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Check if the file exists for the given year\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file_path):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Read the CSV file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     semantic_analysis\u001b[39m.\u001b[39;49mlexicon_nltk(file_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m#semantic_analysis.roberta_semantic_algorithm_twitter(file_path)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile saved for year \u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\semantic_analysis.py:90\u001b[0m, in \u001b[0;36mlexicon_nltk\u001b[1;34m(csv_file)\u001b[0m\n\u001b[0;32m     87\u001b[0m semantic_articles_df[\u001b[39m'\u001b[39m\u001b[39mSentiment scores lexicon\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sentiment_scores\n\u001b[0;32m     89\u001b[0m \u001b[39m# Calculate weighted sentiment values\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m semantic_articles_df[\u001b[39m'\u001b[39m\u001b[39mSentiment value lexicon\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m check_weighted_sum_consistency(semantic_articles_df)\n\u001b[0;32m     92\u001b[0m \u001b[39m# Apply semantic_label to categorize sentiment values\u001b[39;00m\n\u001b[0;32m     93\u001b[0m semantic_articles_df[\u001b[39m'\u001b[39m\u001b[39mSentiment lexicon\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m semantic_articles_df[\u001b[39m'\u001b[39m\u001b[39mSentiment value lexicon\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(semantic_label)\n",
      "File \u001b[1;32mc:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\semantic_analysis.py:166\u001b[0m, in \u001b[0;36mcheck_weighted_sum_consistency\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_weighted_sum_consistency\u001b[39m(df):\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m     \u001b[39m#df['Semantic values roberta twitter'] = df['Semantic values roberta twitter'].apply(lambda x: np.array([float(value) for value in x.strip('[]').split()])) \u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mSemantic scores lexicon\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mSentiment scores lexicon\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: np\u001b[39m.\u001b[39;49marray([\u001b[39mfloat\u001b[39;49m(value) \u001b[39mfor\u001b[39;49;00m value \u001b[39min\u001b[39;49;00m x\u001b[39m.\u001b[39;49mstrip(\u001b[39m'\u001b[39;49m\u001b[39m[]\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msplit()])) \n\u001b[0;32m    167\u001b[0m     coefficients \u001b[39m=\u001b[39m {\n\u001b[0;32m    168\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m    169\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[0;32m    170\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m\n\u001b[0;32m    171\u001b[0m     }\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Initialize a list to store calculated sentiments based on weighted sum\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\semantic_analysis.py:166\u001b[0m, in \u001b[0;36mcheck_weighted_sum_consistency.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_weighted_sum_consistency\u001b[39m(df):\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m     \u001b[39m#df['Semantic values roberta twitter'] = df['Semantic values roberta twitter'].apply(lambda x: np.array([float(value) for value in x.strip('[]').split()])) \u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mSemantic scores lexicon\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mSentiment scores lexicon\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: np\u001b[39m.\u001b[39marray([\u001b[39mfloat\u001b[39m(value) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39;49mstrip(\u001b[39m'\u001b[39m\u001b[39m[]\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit()])) \n\u001b[0;32m    167\u001b[0m     coefficients \u001b[39m=\u001b[39m {\n\u001b[0;32m    168\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m    169\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[0;32m    170\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m\n\u001b[0;32m    171\u001b[0m     }\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Initialize a list to store calculated sentiments based on weighted sum\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2017):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"fox_{year}_output.csv\")\n",
    "    \n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        semantic_analysis.lexicon_nltk(file_path)\n",
    "        #semantic_analysis.roberta_semantic_algorithm_twitter(file_path)\n",
    "        \n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Sentiment scores lexicon' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\merging.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mSentiment values roberta\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m semantic_analysis\u001b[39m.\u001b[39mcheck_weighted_sum_consistency(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m columns \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(df\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m insert_index \u001b[39m=\u001b[39m columns\u001b[39m.\u001b[39;49mindex(\u001b[39m'\u001b[39;49m\u001b[39mSentiment scores lexicon\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# Find the index to insert the new column\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Insert 'Sentiment value roberta' at the appropriate position\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m columns\u001b[39m.\u001b[39minsert(insert_index, \u001b[39m'\u001b[39m\u001b[39mSentiment value roberta\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: 'Sentiment scores lexicon' is not in list"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\fox_{year}_semantics.csv\")\n",
    "\n",
    "    df.columns.values[6] = 'Sentiment label roberta'\n",
    "    \n",
    "    df['Sentiment values roberta'] = semantic_analysis.check_weighted_sum_consistency(df)\n",
    "\n",
    "    columns = list(df.columns)\n",
    "\n",
    "    insert_index = columns.index('Sentiment scores lexicon')  # Find the index to insert the new column\n",
    "    \n",
    "    # Insert 'Sentiment value roberta' at the appropriate position\n",
    "    columns.insert(insert_index, 'Sentiment value roberta')\n",
    "    df = df[columns]  # Reorder columns\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\fox_final_output_{year}.csv\", index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge fox\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "all_data = []\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    file_path = os.path.join(base_path, f\"fox_final_output_{year}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_data.to_csv(os.path.join(base_path, \"fox_reuters_final_output.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "\n",
    "existing_data = pd.read_csv(os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec.csv\"))\n",
    "empty_columns = ['Semantic values roberta twitter', 'Semantic roberta twitter', 'Sentiment value roberta',\n",
    "                'Sentiment scores lexicon', 'Sentiment value lexicon', 'Sentiment lexicon']\n",
    "\n",
    "\n",
    "# Add empty columns with NaN values\n",
    "for col in empty_columns:\n",
    "    existing_data[col] = pd.Series(dtype=object)\n",
    "# Save the updated DataFrame to a new file or overwrite the existing one\n",
    "\n",
    "existing_data.to_csv(os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec_updated.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 13 fields in line 446969, saw 19\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\merging.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39minest\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mOneDrive - Danmarks Tekniske Universitet\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSemester I\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mComputational Tools for Data Science\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mupdated_dataframe_with_clusters_word2vec_updated.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 13 fields in line 446969, saw 19\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\updated_dataframe_with_clusters_word2vec_updated.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File updated for year 2015\n",
      "File updated for year 2016\n",
      "File updated for year 2017\n",
      "File updated for year 2018\n",
      "File updated for year 2019\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "output_file_path = os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec_updated.csv\")\n",
    "# Load existing data\n",
    "existing_data = pd.read_csv(output_file_path)\n",
    "\n",
    "for year in range(2015, 2020):\n",
    "    file_path_2 = os.path.join(base_path, f\"reuters_final_output_{year}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path_2):\n",
    "        new_data = pd.read_csv(file_path_2, usecols=lambda col: col != 'ID')\n",
    "\n",
    "        # Columns to consider in the merge\n",
    "        columns_to_merge = ['Headline', 'Organization', 'Link', 'Date', \n",
    "                            'Semantic values roberta twitter', 'Semantic roberta twitter', \n",
    "                            'Sentiment value roberta', 'Sentiment scores lexicon', \n",
    "                            'Sentiment value lexicon', 'Sentiment lexicon']\n",
    "\n",
    "        # Left merge data based on matching columns\n",
    "        merged_data = pd.merge(existing_data, new_data, on=['Headline', 'Organization', 'Link', 'Date'], how='left')\n",
    "\n",
    "        # Overwrite the existing data with merged data for this year\n",
    "        merged_data.to_csv(output_file_path, index=False)\n",
    "        print(f\"File updated for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\merging.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m year \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2015\u001b[39m, \u001b[39m2020\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df_clusters \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mupdated_dataframe_with_clusters_word2vec_updated.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m# Read the CNN final output for the year\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/inest/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Semester%20I/Computational%20Tools%20for%20Data%20Science/computational_ds/src/semantic_analysis/merging.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df_cnn \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mfr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39minest\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOneDrive - Danmarks Tekniske Universitet\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSemester I\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mComputational Tools for Data Science\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcnn_final_output_\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\inest\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1430\u001b[0m     )\n\u001b[1;32m-> 1433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   1434\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for year in range(2015, 2020):\n",
    "    df_clusters = pd.read_csv(\"updated_dataframe_with_clusters_word2vec_updated.csv\")\n",
    "        \n",
    "        # Read the CNN final output for the year\n",
    "    df_cnn = pd.read_csv(fr\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "\n",
    "        # Merge dataframes based on 'ID' column\n",
    "    merged_df = df_clusters.merge(df_cnn[['ID', 'Semantic values roberta twitter', 'Semantic roberta twitter', 'Sentiment value roberta', 'Sentiment scores lexicon', 'Sentiment value lexicon', 'Sentiment lexicon']], on='ID', how='left')\n",
    "    \n",
    "\n",
    "    merged_df.to_csv(f\"merged_output_{year}.csv\", index=False)\n",
    "\n",
    "    print(f\"Merged data for year {year} saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
