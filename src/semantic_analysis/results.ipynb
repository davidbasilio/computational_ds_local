{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')\n",
    "import  semantic_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at fhamborg/roberta-targeted-sentiment-classification-newsarticles and are newly initialized: ['encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'classifier.out_proj.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'classifier.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'classifier.out_proj.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5731408 0.4268592]\n",
      "[0.56898683 0.43101317]\n",
      "[0.5685259  0.43147406]\n",
      "[0.56615585 0.43384418]\n",
      "[0.56635475 0.43364525]\n",
      "File saved for year 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at fhamborg/roberta-targeted-sentiment-classification-newsarticles and are newly initialized: ['encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'classifier.out_proj.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'classifier.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'classifier.out_proj.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38582787 0.61417216]\n",
      "[0.38706586 0.6129341 ]\n",
      "[0.3863192  0.61368084]\n",
      "[0.39038104 0.60961896]\n",
      "[0.38680845 0.6131916 ]\n",
      "File saved for year 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at fhamborg/roberta-targeted-sentiment-classification-newsarticles and are newly initialized: ['encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'classifier.out_proj.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'classifier.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'classifier.out_proj.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36050007 0.6394999 ]\n",
      "[0.36112934 0.63887066]\n",
      "[0.362389   0.63761103]\n",
      "[0.36154032 0.6384597 ]\n",
      "[0.35877016 0.6412298 ]\n",
      "File saved for year 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at fhamborg/roberta-targeted-sentiment-classification-newsarticles and are newly initialized: ['encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'classifier.out_proj.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'classifier.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'classifier.out_proj.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33073235 0.6692677 ]\n",
      "[0.3351261  0.66487396]\n",
      "[0.33385405 0.6661459 ]\n",
      "[0.33389157 0.66610837]\n",
      "[0.3348793  0.66512066]\n",
      "File saved for year 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at fhamborg/roberta-targeted-sentiment-classification-newsarticles and are newly initialized: ['encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'classifier.out_proj.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'classifier.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'classifier.out_proj.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42349154 0.57650846]\n",
      "[0.42956316 0.57043684]\n",
      "[0.42165902 0.57834095]\n",
      "[0.41876978 0.5812302 ]\n",
      "[0.4247753 0.5752247]\n",
      "File saved for year 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at fhamborg/roberta-targeted-sentiment-classification-newsarticles and are newly initialized: ['encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'classifier.out_proj.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'classifier.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'classifier.out_proj.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52215534 0.4778447 ]\n",
      "[0.5037684 0.4962316]\n",
      "[0.5056984  0.49430162]\n",
      "[0.5145188 0.4854812]\n",
      "[0.50611234 0.49388766]\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "# Define the path\n",
    "base_path = r\"C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\DATA\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"cnn_{year}_output.csv\")\n",
    "    \n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        #semantic_analysis.lexicon_nltk(file_path)\n",
    "        output = semantic_analysis.roberta_semantic_algorithm_news(file_path)\n",
    "        \n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1195, -0.1077]], grad_fn=<AddmmBackward0>)\n",
      "tensor([ 0.1195, -0.1077], grad_fn=<SelectBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([ 0.1195, -0.1077], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the path\n",
    "base_path = r\"C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\DATA\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"cnn_{year}_output.csv\")\n",
    "    \n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        #semantic_analysis.lexicon_nltk(file_path)\n",
    "        output_twitter = semantic_analysis.roberta_semantic_algorithm_twitter(file_path)\n",
    "        \n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
